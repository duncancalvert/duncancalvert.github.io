<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://duncancalvert.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://duncancalvert.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-07-22T15:32:36+00:00</updated><id>https://duncancalvert.github.io/feed.xml</id><title type="html">blank</title><subtitle>My simple, whitespace theme portfolio. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">AI Agent Memory</title><link href="https://duncancalvert.github.io/blog/2025/ai_agent_memory/" rel="alternate" type="text/html" title="AI Agent Memory"/><published>2025-05-30T18:14:10+00:00</published><updated>2025-05-30T18:14:10+00:00</updated><id>https://duncancalvert.github.io/blog/2025/ai_agent_memory</id><content type="html" xml:base="https://duncancalvert.github.io/blog/2025/ai_agent_memory/"><![CDATA[<p><br/> <br/></p> <p style="text-align: center;"> <em>"What we call the present is given shape by an accumulation of memories."</em><br/> — Haruki Murakami </p> <p><br/> <br/></p> <h2 id="why-do-agents-need-memory">Why Do Agents Need Memory</h2> <p>Imagine trying to hold a conversation with someone who forgets everything you’ve said the moment you stop talking. That’s essentially what AI agents are without memory, perpetually amnesiac, doomed to reinvent the wheel with every interaction.</p> <p>Memory gives agents context: what you asked before, what actions they’ve taken, and what worked (or failed) in the past. It’s the difference between your trusted lieutenance and right hand man and a goldfish with Wi-Fi.</p> <p>Just like humans, agents use memory to build up knowledge, learn from past mistakes, and recognize familiar faces. This helps them better personalize experiences, conduct long-term planning, and avoids repeat mistakes. Without memory, an agent can’t improve or adapt; it’s like trying to navigate a city with no map and no recollection of where you’ve been. Memory turns reactive automatons into proactive thinkers.</p> <h2 id="what-is-ai-agent-memory">What is AI Agent Memory</h2> <ul> <li>An agent’s “memory” is data that is not provided by the user in their prompt, but is retrieved and appended to the reasoning process via runtime calls.</li> <li>Agent memory encompasses a diverse set of references and can include everything from past user interactions, previous agent actions, external knowledge bases, system prompts, guardrails, etc.</li> <li>The additional context and knowledge provided by memory helps the agent to better conceptualize the request, plan, and then answer the user or take an action.</li> </ul> <p><br/></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts_agent_memory/agent_memory-480.webp 480w,/assets/img/posts_agent_memory/agent_memory-800.webp 800w,/assets/img/posts_agent_memory/agent_memory-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/posts_agent_memory/agent_memory.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Source: Cognitive Architectures for Language Agents </div> <p><br/></p> <hr/> <h2 id="long-term-agent-memory-types">Long-Term Agent Memory Types</h2> <ol> <li><strong>Episodic:</strong> this type of memory contains past agent interactions and agent action logs. For example, if you asked a chatbot to “repeat the last action that it took in a previous session,” it could use episodic memory to complete this request.</li> <li><strong>Semantic:</strong> this type of memory contains any knowledge the agent should have about itself, or any grounding information stored in knowledge bases that the agent has access to. For example, a vector store in a RAG application is semantic memory.</li> <li><strong>Procedural:</strong> this type of memory contains system information like the system prompt, metadata on available tools, guardrails, etc. It is usually stored and versioned in Git or prompt registry tools.</li> </ol> <hr/> <h2 id="short-term-agent-memory">Short-Term Agent Memory</h2> <ul> <li>Any of the above long-term memory types that is pulled during runtime is called “short-term memory” or “working memory”.</li> <li>This short-term memory is added to the user prompt and passed to the LLM with the aim of boosting performance</li> <li>Any intermediate reasoning steps/action history of the current session is also considered short-term memory when in-use.</li> </ul> <p><br/></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts_agent_memory/ai_agent_gif_cropped-480.webp 480w,/assets/img/posts_agent_memory/ai_agent_gif_cropped-800.webp 800w,/assets/img/posts_agent_memory/ai_agent_gif_cropped-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/posts_agent_memory/ai_agent_gif_cropped.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Source: SwirlAI </div> <p><br/></p> <hr/> <h3 id="references">References</h3> <ul> <li><a href="https://arxiv.org/pdf/2309.02427">Cognitive Architectures for Language Agents</a></li> <li><a href="https://www.newsletter.swirlai.com/">SwirlAI</a></li> </ul>]]></content><author><name></name></author><category term="data-science"/><category term="AI"/><category term="Research"/><category term="Agents"/><summary type="html"><![CDATA[Demystifying the different types of AI agent memory]]></summary></entry><entry><title type="html">The Roadmap to AI Technical Product Manager</title><link href="https://duncancalvert.github.io/blog/2025/the_roadmap_to_ai_tpm/" rel="alternate" type="text/html" title="The Roadmap to AI Technical Product Manager"/><published>2025-05-18T16:04:10+00:00</published><updated>2025-05-18T16:04:10+00:00</updated><id>https://duncancalvert.github.io/blog/2025/the_roadmap_to_ai_tpm</id><content type="html" xml:base="https://duncancalvert.github.io/blog/2025/the_roadmap_to_ai_tpm/"><![CDATA[<p><br/> <br/></p> <p style="text-align: center;"> <em>"Be stubborn on vision but flexible on details."</em><br/> — Jeff Bezos </p> <p><br/> <br/></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts_the_roadmap_to_ai_pm/Map_Denise%20Jans%20Unsplash-480.webp 480w,/assets/img/posts_the_roadmap_to_ai_pm/Map_Denise%20Jans%20Unsplash-800.webp 800w,/assets/img/posts_the_roadmap_to_ai_pm/Map_Denise%20Jans%20Unsplash-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/posts_the_roadmap_to_ai_pm/Map_Denise%20Jans%20Unsplash.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Source: Photo by Denise Jans on Unsplash </div> <p><br/> <br/></p> <h2 id="table-of-contents">Table of Contents</h2> <hr/> <ul> <li><a href="#table-of-contents">Table of Contents</a></li> <li><a href="#what-makes-an-ai-technical-product-manager-different">What Makes an AI Technical Product Manager Different</a></li> <li><a href="#skill-sets">Skill Sets</a> <ul> <li><a href="#traditional-data-science">Traditional Data Science</a></li> <li><a href="#gen-ai--foundation-models">Gen AI &amp; Foundation Models</a></li> <li><a href="#retrieval-augmented-generation">Retrieval Augmented Generation</a></li> <li><a href="#agentic-ai">Agentic AI</a></li> <li><a href="#general-technical-skills">General Technical Skills</a></li> <li><a href="#ai-product-skills">AI Product Skills</a></li> <li><a href="#general-product-management">General Product Management</a></li> </ul> </li> <li><a href="#newsletters-podcasts-and-people">Newsletters, Podcasts, and People</a> <ul> <li><a href="#newsletters">Newsletters</a></li> <li><a href="#podcasts">Podcasts</a></li> <li><a href="#people-to-follow">People to Follow</a></li> </ul> </li> </ul> <h2 id="what-makes-an-ai-technical-product-manager-different">What Makes an AI Technical Product Manager Different</h2> <p>The rapid rise of AI and Agentic frameworks has redefined the landscape of product development and the role of a TPM. A traditional TPM focuses on building scalable systems, ensures alignment of engineering with product, and translates business needs into technical specs. They primarily build with tools that are time-tested, relatively stable, and well documented.</p> <p>In contrast, AI TPMs live at the intersection of infrastructure, cutting edge research, and business outcomes. They guide the development of probabilistic, data-dependent products where performance varies widely across inputs, and success isn’t measured in “features shipped” but in hard to measure metrics like model quality, inference efficiency, and real-world generalization. Add to that the fact that tools are shifting under their feet with vendors and the open-source community launching new frameworks based on the latest cutting edge agentic and Gen AI research.</p> <p>Instead of the generally linear Software Development Lifecycle (SDLC) used to build traditional products, building AI systems require a highly iterative Model Development Lifecycle (MDLC). This involves continuously tweaking model training pipelines, feature stores, real-time inference latency, versioning of data/models, and monitoring for drift and degradation. This must be done in alignment with a motely crew of data scientists, ML engineers, infra teams, data SMEs, end users, and model governance stakeholders.</p> <p><br/></p> <h2 id="skill-sets">Skill Sets</h2> <hr/> <h3 id="traditional-data-science">Traditional Data Science</h3> <details> <summary><b>AI Model Theory</b></summary> <ul> <li>Understand the difference between Random Forest, SVM, and KNN and when to use one over the other on a problem.</li> <li>Understand the model architecture, hyperparameters, and issues with each model.</li> </ul> </details> <details> <summary><b>AI Evaluation Metrics</b></summary> <ul> <li>Build an intuitive understanding of the right metrics for each model and use case.</li> <li>Recognize areas of concern or blind spots for each metric.</li> </ul> </details> <details> <summary><b>ML &amp; Deep Learning Frameworks (Scikit-Learn, PyTorch, TensorFlow)</b></summary> <ul> <li><a href="https://www.coursera.org/specializations/deep-learning">(Class) Deep Learning Specialisation by Andrew Ng</a></li> <li><a href="https://www.deeplearningbook.org/">(Book) Deep Learning by Ian Goodfellow</a></li> <li><a href="https://www.oreilly.com/library/view/hands-on-machine-learning/9781098125967/">(Book) Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 3rd Edition by Aurélien Géron</a></li> <li><a href="https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ">(Videos) Neural Networks: Zero to Hero by Andrej Karpathy</a></li> </ul> </details> <details> <summary><b>Model Development Lifecycle (MDLC)</b></summary> <ul> <li>Understand the end-to-end process of building, testing, deploying, and monitoring machine learning models.</li> </ul> </details> <details> <summary><b>Machine Learning Operations (MLOps)</b></summary> <ul> <li>Learn the principles and practices of maintaining and scaling ML workflows in production environments.</li> <li>General Resources</li> <ul> <li><a href="https://www.amazon.com/Designing-Machine-Learning-Systems-Production-Ready/dp/1098107969">(Book) Designing Machine Learning Systems by Chip Huyen</a></li> <li><a href="https://github.com/GokuMohandas/Made-With-ML">(Class) Made With ML</a></li> </ul> </ul> </details> <details> <summary><b>Python</b></summary> <ul> <li>Learn Object-oriented programming (OOP) principles.</li> <li>Proficiency in Pandas and NumPy for data manipulation.</li> <li>Use Jupyter notebooks for exploration and experimentation.</li> </ul> </details> <details> <summary><b>SQL</b></summary> <ul> <li>Ensure fluency in querying and manipulating structured data from relational databases.</li> </ul> </details> <details> <summary><b>Apache Spark / PySpark</b></summary> <ul> <li>Leverage distributed computing for large-scale data processing.</li> <li>Use PySpark for writing scalable, Python-based ETL and analysis pipelines.</li> </ul> </details> <details> <summary><b>Data Warehouse / Data Lakehouse</b></summary> <ul> <li>Databricks</li> <li>Snowflake</li> <li>GCP BigQuery</li> </ul> </details> <hr/> <h3 id="gen-ai--foundation-models">Gen AI &amp; Foundation Models</h3> <details> <summary><b>Cloud Model APIs</b></summary> <ul> <li><a href="https://azure.microsoft.com/en-us/products/ai-model-catalog">Azure - AI Foundry</a></li> <li><a href="https://cloud.google.com/model-garden">GCP - Vertex AI Model Garden</a></li> <li><a href="https://aws.amazon.com/bedrock/">AWS - Amazon Bedrock</a></li> <li><a href="https://openai.com/api/">OpenAI</a></li> </ul> </details> <details> <summary><b>Transformers</b></summary> <ul> <li>Attention Mechanism</li> <li>Positional Encoding</li> <li>Tokenization &amp; Embeddings</li> <li>Decoder-Only, Encoder-Only, and Encoder-Decoder Architecture</li> <li>Hyperparameter Tuning</li> <ul> <li>Temperature</li> <li>top-K</li> <li>top-P</li> </ul> <li>Resources:</li> <ul> <li><a href="https://www.manning.com/books/build-a-large-language-model-from-scratch">(Book) Build a Large Language Model (From Scratch) by Sebastian Raschka</a></li> <li><a href="https://www.amazon.com/Hands-Large-Language-Models-Understanding/dp/1098150961">(Book) Hands-On Large Language Models by Jay Alammar</a></li> <li><a href="https://www.amazon.com/Natural-Language-Processing-Transformers-Revised/dp/1098136799">(Book) Natural Language Processing with Transformers by Lewis Tunstall</a></li> <li><a href="https://www.youtube.com/watch?v=7xTGNNLPyMI&amp;ab_channel=AndrejKarpathy">(Video) Deep Dive into LLMs like ChatGPT by Andrej Karpathy</a></li> <li><a href="https://arxiv.org/abs/1706.03762">(Paper) Attention is All You Need</a></li> <li><a href="https://www.youtube.com/watch?v=9vM4p9NN0Ts&amp;ab_channel=StanfordOnline">(Class) Stanford CS229 - Machine Learning - Building Large Language Models (LLMs)</a></li> </ul> </ul> </details> <details> <summary><b>Diffusion Models</b></summary> </details> <details> <summary><b>Generative Adversarial Networks (GAN)</b></summary> </details> <details> <summary><b>LLM Fine-Tuning</b></summary> <ul> <li>Compute Efficiency Techniques</li> <ul> <li>LoRA</li> <li>QLoRA</li> <li>PEFT</li> </ul> </ul> </details> <details> <summary><b>LLM Benchmarks</b></summary> <ul> <li><a href="https://www.latent.space/p/benchmarks-101">(Podcast) AI Fundamentals: Benchmarks 101</a></li> <li><a href="https://www.latent.space/p/benchmarks-201">(Podcast) Benchmarks 201: Why Leaderboards &gt; Arenas &gt;&gt; LLM-as-Judge</a></li> </ul> </details> <details> <summary><b>LLM Evaluation</b></summary> <ul> <li>LLM Eval Metrics</li> <ul> <li>Statistical Metrics</li> <ul> <li><a href="https://en.wikipedia.org/wiki/BLEU">BLEU</a></li> <li><a href="https://en.wikipedia.org/wiki/ROUGE_(metric)">ROUGE (Recall-Oriented Understudy for Gisting Evaluation)</a></li> <li><a href="https://en.wikipedia.org/wiki/METEOR">METEOR (Metric for Evaluation of Translation with Explicit Ordering)</a></li> <li><a href="https://en.wikipedia.org/wiki/Levenshtein_distance">Levenshtein Distance</a></li> </ul> <li>LLM-as-Judge Metrics</li> </ul> <li>LLM Eval Tools</li> <ul> <li><a href="https://docs.ragas.io">Ragas</a></li> </ul> <li>LLM-as-Judge Techniques</li> <ul> <li>Pairwise Comparison</li> <li>Evaluation by Criteria (Reference Free)</li> <li>Evaluation by Criteria (Reference-Based)</li> </ul> </ul> </details> <details> <summary><b>LLM Observability</b></summary> <ul> <li>Langfuse</li> <li><a href="https://www.langchain.com/langsmith">LangSmith</a>: a developer platform for inspecting, tracing, and evaluating LLM-powered applications built with LangChain or other orchestration frameworks. It enables fine-grained logging of prompts, model inputs/outputs, tool invocations, and intermediate steps, while supporting automated and manual evaluation workflows for performance, latency, and correctness.</li> </ul> </details> <details> <summary><b>LLM Interpretability</b></summary> <ul> <li>Anthropic's Interpretability Team</li> <ul> <li><a href="https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html">Dictionary Learning</a></li> <li><a href="https://www.anthropic.com/research/towards-monosemanticity-decomposing-language-models-with-dictionary-learning">Monosemanticity</a></li> <li><a href="https://transformer-circuits.pub/2025/attribution-graphs/biology.html">Attributional Graphs</a></li> </ul> </ul> </details> <details> <summary><b>LLM Safety &amp; Security</b></summary> <ul> <li>Content Filtering</li> <ul> <li><a href="https://cloud.google.com/security-command-center/docs/model-armor-overview">GCP Model Armor</a> </li> </ul> </ul> </details> <details> <summary><b>Prompt/Context Engineering</b></summary> <ul> <li><a href="https://platform.openai.com/docs/guides/text?api-mode=responses">(Article) OpenAI Prompting Guide</a></li> <li><a href="https://www.promptingguide.ai/">(Website) Prompt Engineering Guide by DAIR.AI</a></li> </ul> </details> <details> <summary><b>AI Engineering</b></summary> <ul> <li><a href="https://www.amazon.com/AI-Engineering-Building-Applications-Foundation/dp/1098166302">(Book) AI Engineering: Building Applications with Foundation Models by Chip Huyen</a></li> </ul> </details> <hr/> <h3 id="retrieval-augmented-generation">Retrieval Augmented Generation</h3> <details> <summary><b>RAG Fundamentals</b></summary> <ul> <li>Vector embeddings</li> <li>Chunking</li> <li>Hybrid retrieval</li> <li>General Resources</li> <ul> <li> <a href="https://github.com/NirDiamant/RAG_Techniques">(GitHub) RAG Techniques by Nir Diamant</a> </li> </ul> </ul> </details> <details> <summary><b>RAG Evaluation</b></summary> <ul> <li>RAG Eval Metrics</li> <ul> <li>Context Precision</li> <li>Context Recall</li> <li>Content Entities Recall</li> <li>Noise Sensitivity</li> <li>Response Relevance</li> <li>Faithfulness</li> <li>Multimodal Faithfulness</li> <li>Multimodal Relevance</li> </ul> <li>RAG Eval Tools</li> <ul> <li>Ragas</li> </ul> </ul> </details> <details> <summary><b>Vector Databases</b></summary> <ul> <li>Vector Search Prototyping Libraries</li> <ul> <li><a href="https://faiss.ai/">FAISS</a></li> <li><a href="https://github.com/nmslib/hnswlib">HNSWlib</a></li> </ul> <li>Production Databases</li> <ul> <li>Pinecone</li> <li>Weaviate</li> <li>Chroma</li> <li>Elasticsearch</li> <li>Milvus</li> </ul> </ul> </details> <hr/> <h3 id="agentic-ai">Agentic AI</h3> <details> <summary><b>Agent Fundamentals</b></summary> <ul> <li><a href="https://www.kaggle.com/whitepaper-agents">(White Paper) Google Agents White Paper by Julia Wiesinger et al.</a></li> <li><a href="https://www.kaggle.com/whitepaper-agent-companion">(White Paper) Google Agents Companion by Antonio Gulli et al.</a></li> <li><a href="https://arxiv.org/abs/2505.10468">(Paper) AI Agents vs. Agentic AI: A Conceptual Taxonomy, Applications and Challenges</a></li> <li><a href="https://arxiv.org/abs/2210.03629">(Paper) ReAct: Synergizing Reasoning and Acting in Language Models by Shunyu Yao et al.</a></li> <li><a href="https://huggingface.co/learn/agents-course/en/unit0/introduction">(Course) HuggingFace AI Agents Course</a></li> </ul> </details> <details> <summary><b>Agent Evals</b></summary> <ul> <li><a href="https://arxiv.org/abs/2410.10934">(Paper) Agent-as-a-Judge: Evaluate Agents with Agents</a></li> </ul> </details> <details> <summary><b>Agent Frameworks</b></summary> <ul> <li>LangChain</li> <li>LangGraph <ul> <li><a href="https://www.deeplearning.ai/short-courses/ai-agents-in-langgraph/">(Class) AI Agents in LangGraph by DeepLearning.AI</a></li> </ul> </li> <li>LlamaIndex <ul> <li><a href="https://www.deeplearning.ai/short-courses/building-agentic-rag-with-llamaindex/">(Class) Building Agentic RAG with LlamaIndex</a></li> </ul> </li> <li>OpenAI Agent SDK</li> <li>Mastra</li> </ul> </details> <details> <summary><b>Agent Protocols</b></summary> <ul> <li>Anthropic Model Context Protocol (MCP) <ul> <li><a href="https://blog.neosage.io/p/why-every-ai-builder-needs-to-understand">(Article) Why Every AI Builder Needs to Understand MCP</a></li> </ul> </li> <li>Google Agent-2-Agent (A2A)</li> </ul> </details> <details> <summary><b>AI Integrated Development Environments (IDEs)</b></summary> <ul> <li>Cursor</li> <li>Windsurf</li> <li>Replit</li> </ul> </details> <details> <summary><b>Agentic Design Patterns</b></summary> <ul> <li><a href="https://www.philschmid.de/agentic-pattern">(Article) Zero to One: Learning Agentic Patterns</a></li> <li><a href="https://www.deeplearning.ai/short-courses/ai-agentic-design-patterns-with-autogen/">(Class) AI Agentic Design Patterns with AutoGen</a></li> <li><a href="https://www.deeplearning.ai/short-courses/multi-ai-agent-systems-with-crewai/">(Class) Multi AI Agent Systems with crewAI</a></li> </ul> </details> <hr/> <h3 id="general-technical-skills">General Technical Skills</h3> <details> <summary><b>Public Cloud Infrastructure</b></summary> <ul> <li><a href="https://cloud.google.com/">Google Cloud Platform (GCP)</a></li> <li><a href="https://azure.microsoft.com/en-us/">Microsoft Azure</a></li> <li><a href="https://aws.amazon.com/">Amazon Web Services (AWS)</a></li> </ul> </details> <details> <summary><b>Data Pipelines</b></summary> <ul> <li>Apache Airflow</li> <ul> <li>GCP Composer</li> <li>Amazon Managed Workflows for Apache Airflow (MWAA)</li> <li>Azure Workflow Orchestration Manager</li> </ul> <li>Dataflow</li> <li>Apache Beam</li> <li>Apache Kafka</li> </ul> </details> <details> <summary><b>API and Backend Skills</b></summary> <ul> <li>Develop backends with FastAPI or Flask</li> <li>Implement REST and streaming endpoints for AI services</li> <li>Design authentication and rate-limiting systems</li> <li>Build WebSocket implementations for real-time AI interactions</li> </ul> </details> <hr/> <h3 id="ai-product-skills">AI Product Skills</h3> <details> <summary><b>AI Product Sense</b></summary> <ul> <li>Understand what can, and importantly cannot, be solved by AI (i.e. AI is not a silver bullet, many processes and products are better served with non-AI solutions)</li> </ul> </details> <details> <summary><b>AI Experiment Design</b></summary> <ul> <li>Practice iterative hypothesis testing with quantitative evaluation. </li> <li>Lead with A/B test, user interviews, and user feedback loops where possible</li> </ul> </details> <details> <summary><b>Market Insight</b></summary> <ul> <li>Build a deep understanding of the AI market, its competitive landscape, and emerging trends</li> </ul> </details> <details> <summary><b>User Journeys</b></summary> <ul> <li>Define clear user journeys aligned with a strategic AI product philosophy and a north star metric.</li> </ul> </details> <hr/> <h3 id="general-product-management">General Product Management</h3> <details> <summary><b>Project Management Frameworks</b></summary> <ul> <li><ins>Waterfall</ins>: A traditional, sequential approach where each project phase is completed before the next begins. Each phase has specific deliverables and a review process, making it suitable for projects with clearly defined requirements and predictable outcomes. However, it offers limited flexibility for changes once a phase is complete.</li> <li><ins>Agile</ins>: An iterative and incremental approach, suitable for projects with evolving requirements</li> <ul> <li><ins>Scrum</ins>: structured roles, sprints, and ceremonies.</li> <li><ins>Kanban</ins>: visual flow-based system emphasizing WIP limits and continuous delivery.</li> </ul> </ul> </details> <details> <summary><b>Continuous Integration / Continuous Delivery (CI/CD)</b></summary> <ul> <li>Automate testing, building, and deployment to speed up release cycles and improve reliability.</li> </ul> </details> <details> <summary><b>DevOps and Site Reliability Engineering (SRE)</b></summary> <ul> <li>Bridge development and operations to ensure scalable, stable, and reliable systems.</li> <li>SRE focuses on uptime, latency, monitoring, and incident response with a software engineering mindset.</li> </ul> </details> <details> <summary><b>Robust Documentation</b></summary> <ul> <li>Ensure product documentation is clear, current, and accessible to cross-functional teams.</li> </ul> </details> <details> <summary><b>FinOps</b></summary> <ul> <li>Manage cloud financial operations to maximize efficiency and optimize cost.</li> </ul> </details> <details> <summary><b>Stakeholder Management</b></summary> <ul> <li>Adept at influencing executives and building consensus in a constantly changing and fast-paced environment.</li> </ul> </details> <details> <summary><b>Expert Storytelling</b></summary> <ul> <li>Craft compelling product messaging and present effectively to diverse audiences.</li> </ul> </details> <details> <summary><b>Product Launch Experience</b></summary> <ul> <li>Know what to do at each product launch stage and how to execute effectively to get things over the finish line</li> </ul> </details> <details> <summary><b>Growth and Expansive Mindset</b></summary> <ul> <li>Foster a curiosity to learn, a growth mindset, a positive attitude, and a "kind human" policy.</li> </ul> </details> <p><br/></p> <h2 id="newsletters-podcasts-and-people">Newsletters, Podcasts, and People</h2> <hr/> <h3 id="newsletters">Newsletters</h3> <ul> <li><a href="https://thesequence.substack.com/">The Sequence</a>: A weekly series that does technical deep dives on the latest AI/ML techniques</li> <li><a href="https://www.deeplearning.ai/the-batch/">The Batch @ DeepLearning.AI</a>: a weekly deep dive from Stanford Professor Andrew Ng</li> <li><a href="https://www.deeplearning.ai/the-batch/tag/data-points/">Data Points</a></li> <li><a href="https://www.dailyzaps.com/">Daily Zaps</a>: high level tech news, not very technical</li> <li><a href="https://mlops.substack.com/">The MLOps Newsletter</a>: technical with a specific focus on MLOps</li> <li><a href="https://developers.google.com/newsletter">Google Developer Program</a>: stay up to date with the latest GCP releases</li> <li><a href="https://medium.com/towards-data-science/newsletter">The Variable</a>: a curated list of articles/tutorials from Towards Data Science, the data science channel in Medium</li> <li><a href="https://www.technologyreview.com/topic/download-newsletter/">The Download from MIT Technology Review</a>: a higher level tech news roundup</li> <li><a href="https://www.turingpost.com/subscribe?ref=WAGU23hEVa">Turing Post</a></li> <li><a href="https://www.newsletter.swirlai.com/">SwirlAI</a>: MLOps and data engineering focused newsletter with great visualizations</li> </ul> <h3 id="podcasts">Podcasts</h3> <ul> <li><a href="https://podcasts.apple.com/us/podcast/practical-ai/id1406537385">Practical AI by Changelog</a></li> <li><a href="https://www.youtube.com/playlist?list=PLRRoCwK1ZTNCAZXXOswpIYQqzMgT4swsI">Inference by Turing Post</a></li> <li><a href="https://www.latent.space/podcast">Latent Space: The AI Engineer Podcast</a></li> </ul> <h3 id="people-to-follow">People to Follow</h3> <ul> <li><a href="https://www.linkedin.com/in/yann-lecun/">Yann LeCun</a>: Chief AI Scientist at Meta and a pioneer in optical character recognition (OCR) and convolutional neural networks (CNN). A Turing Award winner and one of the three “Godfathers of AI”.</li> <li><a href="https://karpathy.ai/">Andrej Karpathy</a>: Former director of Autopilot at Tesla, co-founder of OpenAI, and prolific AI educator.</li> <li><a href="https://www.linkedin.com/in/fei-fei-li-4541247/">Fei-Fei Li</a>: Stanford CS professor, co-director of the Stanford Institute for Human-Centered AI, inventor of ImageNet, and former Chief Scientist of AI/ML at GCP.</li> <li><a href="https://eugeneyan.com/subscribe">Eugene Yan</a>: ML, RecSys, LLMs, and engineering</li> <li><a href="https://www.andrewng.org/">Andrew Ng</a>: founder of Coursera, DeepLearning.AI, Stanford AI computer science professor, and neural network pioneer</li> </ul>]]></content><author><name></name></author><category term="data-science"/><category term="AI"/><category term="Machine_Learning"/><category term="Deep_Learning"/><category term="Research"/><category term="Neural_Networks"/><category term="Product_Management"/><category term="Agents"/><summary type="html"><![CDATA[A guide to AI upskilling for Technical Product Managers]]></summary></entry><entry><title type="html">RAG Engine Optimization: An Emerging Discipline</title><link href="https://duncancalvert.github.io/blog/2024/rag-engine-optimization-an-emerging-discipline/" rel="alternate" type="text/html" title="RAG Engine Optimization: An Emerging Discipline"/><published>2024-10-29T21:50:11+00:00</published><updated>2024-10-29T21:50:11+00:00</updated><id>https://duncancalvert.github.io/blog/2024/rag-engine-optimization-an-emerging-discipline</id><content type="html" xml:base="https://duncancalvert.github.io/blog/2024/rag-engine-optimization-an-emerging-discipline/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">How to Pass the Neo4j Certified Professional Exam</title><link href="https://duncancalvert.github.io/blog/2024/how-to-pass-the-neo4j-certified-professional-exam/" rel="alternate" type="text/html" title="How to Pass the Neo4j Certified Professional Exam"/><published>2024-08-04T19:52:57+00:00</published><updated>2024-08-04T19:52:57+00:00</updated><id>https://duncancalvert.github.io/blog/2024/how-to-pass-the-neo4j-certified-professional-exam</id><content type="html" xml:base="https://duncancalvert.github.io/blog/2024/how-to-pass-the-neo4j-certified-professional-exam/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">The Platonic Representation Hypothesis</title><link href="https://duncancalvert.github.io/blog/2024/the_platonic_representation_hypothesis/" rel="alternate" type="text/html" title="The Platonic Representation Hypothesis"/><published>2024-05-29T05:36:10+00:00</published><updated>2024-05-29T05:36:10+00:00</updated><id>https://duncancalvert.github.io/blog/2024/the_platonic_representation_hypothesis</id><content type="html" xml:base="https://duncancalvert.github.io/blog/2024/the_platonic_representation_hypothesis/"><![CDATA[<p>Amidst the buzz around GPT-4o and Google I/O, it was easy to miss this fascinating philosophical paper from MIT. “<a href="https://phillipi.github.io/prh/">The Platonic Representation Hypothesis</a>,” argues that AI models are converging towards a shared “reality”. As they grow in size, their data representations across different modalities (images, text, etc.) become more aligned, echoing Plato’s ideal forms.</p> <p><strong>Highlights:</strong></p> <ul> <li>Unified Representations: Larger models show more alignment, suggesting a common underlying reality and statistical model.</li> <li>Multimodal Efficiency: Combining data types boosts model performance and convergence.</li> <li>Revolutionary Potential: Easier cross-modal translation and adaptation could transform AI applications.</li> </ul>]]></content><author><name></name></author><category term="data-science"/><category term="AI"/><category term="MachineLearning"/><category term="DeepLearning"/><category term="MIT"/><category term="Research"/><category term="NeuralNetworks"/><summary type="html"><![CDATA[Exploring world representations in neural networks]]></summary></entry><entry><title type="html">Building Efficient Baselines in Minutes: EDA and Classification in Google Colab</title><link href="https://duncancalvert.github.io/blog/2023/building-efficient-baselines-in-minutes-eda-and-classification-in-google-colab/" rel="alternate" type="text/html" title="Building Efficient Baselines in Minutes: EDA and Classification in Google Colab"/><published>2023-11-26T23:23:35+00:00</published><updated>2023-11-26T23:23:35+00:00</updated><id>https://duncancalvert.github.io/blog/2023/building-efficient-baselines-in-minutes-eda-and-classification-in-google-colab</id><content type="html" xml:base="https://duncancalvert.github.io/blog/2023/building-efficient-baselines-in-minutes-eda-and-classification-in-google-colab/"><![CDATA[]]></content><author><name></name></author></entry></feed>