<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://duncancalvert.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://duncancalvert.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-01-04T22:23:45+00:00</updated><id>https://duncancalvert.github.io/feed.xml</id><title type="html">blank</title><subtitle>My simple, whitespace theme portfolio. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">AI Agent Memory</title><link href="https://duncancalvert.github.io/blog/2025/ai_agent_memory/" rel="alternate" type="text/html" title="AI Agent Memory"/><published>2025-05-30T18:14:10+00:00</published><updated>2025-05-30T18:14:10+00:00</updated><id>https://duncancalvert.github.io/blog/2025/ai_agent_memory</id><content type="html" xml:base="https://duncancalvert.github.io/blog/2025/ai_agent_memory/"><![CDATA[<p><br/> <br/></p> <p style="text-align: center;"> <em>"What we call the present is given shape by an accumulation of memories."</em><br/> — Haruki Murakami </p> <p><br/> <br/></p> <h2 id="why-do-agents-need-memory">Why Do Agents Need Memory</h2> <p>Imagine trying to hold a conversation with someone who forgets everything you’ve said the moment you stop talking. That’s essentially what AI agents are without memory, perpetually amnesiac, doomed to reinvent the wheel with every interaction.</p> <p>Memory gives agents context: what you asked before, what actions they’ve taken, and what worked (or failed) in the past. It’s the difference between your trusted lieutenance and right hand man and a goldfish with Wi-Fi.</p> <p>Just like humans, agents use memory to build up knowledge, learn from past mistakes, and recognize familiar faces. This helps them better personalize experiences, conduct long-term planning, and avoids repeat mistakes. Without memory, an agent can’t improve or adapt; it’s like trying to navigate a city with no map and no recollection of where you’ve been. Memory turns reactive automatons into proactive thinkers.</p> <h2 id="what-is-ai-agent-memory">What is AI Agent Memory</h2> <ul> <li>An agent’s “memory” is data that is not provided by the user in their prompt, but is retrieved and appended to the reasoning process via runtime calls.</li> <li>Agent memory encompasses a diverse set of references and can include everything from past user interactions, previous agent actions, external knowledge bases, system prompts, guardrails, etc.</li> <li>The additional context and knowledge provided by memory helps the agent to better conceptualize the request, plan, and then answer the user or take an action.</li> </ul> <p><br/></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts_agent_memory/agent_memory-480.webp 480w,/assets/img/posts_agent_memory/agent_memory-800.webp 800w,/assets/img/posts_agent_memory/agent_memory-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/posts_agent_memory/agent_memory.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Source: Cognitive Architectures for Language Agents </div> <p><br/></p> <hr/> <h2 id="long-term-agent-memory-types">Long-Term Agent Memory Types</h2> <ol> <li><strong>Episodic:</strong> this type of memory contains past agent interactions and agent action logs. For example, if you asked a chatbot to “repeat the last action that it took in a previous session,” it could use episodic memory to complete this request.</li> <li><strong>Semantic:</strong> this type of memory contains any knowledge the agent should have about itself, or any grounding information stored in knowledge bases that the agent has access to. For example, a vector store in a RAG application is semantic memory.</li> <li><strong>Procedural:</strong> this type of memory contains system information like the system prompt, metadata on available tools, guardrails, etc. It is usually stored and versioned in Git or prompt registry tools.</li> </ol> <hr/> <h2 id="short-term-agent-memory">Short-Term Agent Memory</h2> <ul> <li>Any of the above long-term memory types that is pulled during runtime is called “short-term memory” or “working memory”.</li> <li>This short-term memory is added to the user prompt and passed to the LLM with the aim of boosting performance</li> <li>Any intermediate reasoning steps/action history of the current session is also considered short-term memory when in-use.</li> </ul> <p><br/></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts_agent_memory/ai_agent_gif_cropped-480.webp 480w,/assets/img/posts_agent_memory/ai_agent_gif_cropped-800.webp 800w,/assets/img/posts_agent_memory/ai_agent_gif_cropped-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/posts_agent_memory/ai_agent_gif_cropped.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Source: SwirlAI </div> <p><br/></p> <hr/> <h3 id="references">References</h3> <ul> <li><a href="https://arxiv.org/pdf/2309.02427">Cognitive Architectures for Language Agents</a></li> <li><a href="https://www.newsletter.swirlai.com/">SwirlAI</a></li> </ul>]]></content><author><name></name></author><category term="data-science"/><category term="AI"/><category term="Research"/><category term="Agents"/><summary type="html"><![CDATA[Demystifying the different types of AI agent memory]]></summary></entry><entry><title type="html">The Roadmap to AI Product Manager</title><link href="https://duncancalvert.github.io/blog/2025/the_roadmap_to_ai_pm/" rel="alternate" type="text/html" title="The Roadmap to AI Product Manager"/><published>2025-05-18T16:04:10+00:00</published><updated>2025-05-18T16:04:10+00:00</updated><id>https://duncancalvert.github.io/blog/2025/the_roadmap_to_ai_pm</id><content type="html" xml:base="https://duncancalvert.github.io/blog/2025/the_roadmap_to_ai_pm/"><![CDATA[<p><br/> <br/></p> <p style="text-align: center;"> <em>"Be stubborn on vision but flexible on details."</em><br/> — Jeff Bezos </p> <p><br/> <br/></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts_the_roadmap_to_ai_pm/Map_Denise%20Jans%20Unsplash-480.webp 480w,/assets/img/posts_the_roadmap_to_ai_pm/Map_Denise%20Jans%20Unsplash-800.webp 800w,/assets/img/posts_the_roadmap_to_ai_pm/Map_Denise%20Jans%20Unsplash-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/posts_the_roadmap_to_ai_pm/Map_Denise%20Jans%20Unsplash.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Source: Photo by Denise Jans on Unsplash </div> <p><br/> <br/></p> <h2 id="table-of-contents">Table of Contents</h2> <h2 id="what-makes-an-ai-product-manager-different">What Makes an AI Product Manager Different</h2> <p>The rapid advances in AI technology in the last few years have redefined the product development landscape and given rise to the new role of a AI Product Manager (PM). A traditional PM focuses on building scalable systems, ensures alignment of engineering with product, and translates business needs into technical specs. They primarily build with tools that are time-tested, relatively stable, and well documented.</p> <p>In contrast, AI TPMs live at the intersection of infrastructure, cutting edge research, and business outcomes. They guide the development of probabilistic, data-dependent products where performance varies widely across inputs, and success isn’t measured in “features shipped” but in hard to measure metrics like model quality, inference efficiency, and real-world generalization. Add to that the fact that tools are shifting under their feet with vendors and the open-source community launching new frameworks based on the latest cutting edge agentic and Gen AI research.</p> <p>Instead of the generally linear Software Development Lifecycle (SDLC) used to build traditional products, building AI systems require a highly iterative Model Development Lifecycle (MDLC). This involves continuously tweaking model training pipelines, feature stores, real-time inference latency, versioning of data/models, and monitoring for drift and degradation. This must be done in alignment with a motely crew of data scientists, ML engineers, infra teams, data SMEs, end users, and model governance stakeholders.</p> <p>(blank line)</p> <h2 id="skill-sets">Skill Sets</h2> <hr/> <h3 id="traditional-data-science">Traditional Data Science</h3> <details> <summary><b>Probability &amp; Statistics</b></summary> <ul> <li><a href="https://www.oreilly.com/library/view/practical-statistics-for/9781492072935/">(Book) Practical Statistics for Data Scientists by Peter Bruce</a></li> </ul> </details> <details> <summary><b>Machine Learning Theory</b></summary> <ul> <li>Supervised vs. unsupervised learning</li> <li>Regression, classification, clustering, association, and dimensionality reduction</li> <li>Familiarity with statistics and ML models and when to use one over the other</li> <ul> <li>Linear Regression</li> <li>Logistic Regression</li> <li>Random Forest</li> <li>Support Vector Machines (SVM)</li> <li>eXtreme Gradient Boosting (XGBoost)</li> <li>K-means Clustering</li> <li>Light Gradient Boosting Machine (LightGBM)</li> <li>Categorical Boosting (CatBoost)</li> </ul> <li>Model hyperparameters</li> <li>Grid vs. random search (gradient descent)</li> <li>Loss functions</li> <li>Overfitting and underfitting</li> <li>Bias and variance</li> <li>Model evaluation techniques (train/test split, cross validation, metrics)</li> </ul> </details> <details> <summary><b>Deep Learning Theory</b></summary> <ul> <li>Model architectures for Artificial Neural Networks (ANN) and deep learning models (DNN, CNN, RNN, etc.)</li> <ul> <li>Activation functions</li> <ul> <li>Rectified Linear Unit (ReLU)</li> <ul> <li>Leaky ReLU</li> <li>Softplus/SmoothReLU</li> <li>Parametric ReLU (PReLU)</li> <li>Exponential Linear Unit (ELU)</li> <li>Gaussian Error Linear Unit (GELU)</li> </ul> <li>Sigmoid</li> </ul> <li>Loss functions/cost functions</li> <ul> <li>SME</li> </ul> <li>Optimizers</li> <ul> <li>Adadelta</li> <li>Adafactor</li> <li>Adaptive Gradient Algorithm (AdaGrad)</li> <li>Adaptive Moment Estimation (Adam)</li> <li>Adam with Weight Decay (AdamW)</li> <li>Adamax</li> <li>Follow-the-Regularized-Leader (Ftrl)</li> <li>Lion</li> <li>LossScaleOptimizer</li> <li>Nesterov-accelerated Adaptive Moment Estimation (Nadam)</li> <li>Root Mean Square Propagation (RMSprop)</li> <li>Stochastic Gradient Descent (SGD)</li> </ul> <li>Backpropagation</li> <li>Epochs</li> <li>Learning Rate</li> <li>Batch Size</li> <li>Regularization</li> <ul> <li>Early Stopping</li> <li>Parameter Norm Penalties <ul> <li>L1 Regularization</li> <li>L2 Regularization</li> <li>Max-norm Regularization</li> </ul> </li> <li>Dataset Augmentation</li> <li>Noise Robustness</li> <li>Sparse Representations</li> </ul> <li>Model weights and biases</li> <li><ins>Embedding dimensionality</ins>: Lower dimensions can lead to less accuracy, more lossy compression. Higher can lead to overfitting and slow training time. A good starting point for the number of embedding dimensions is Dimensions \(\approx \sqrt[4]{\mathrm{Possible\ values}}\)</li> </ul> </ul> </details> <details> <summary><b>AI Evaluation Metrics</b></summary> <ul> <li>Theory</li> <ul> <li>Build an intuitive understanding of the right metrics for each model and use case.</li> <li>Recognize areas of concern or blind spots for each metric.</li> </ul> <li>Regression Eval Metrics &amp; Techniques</li> <ul> <li>Root Mean Squared Error (RMSE)</li> </ul> <li>Classification Eval Metrics &amp; Techniques</li> <ul> <li>Accuracy</li> <li>Precision</li> <li>Precision</li> <li>Precision vs. Recall Curve</li> <li>Area Under the Curve (AUC)</li> <li>Receiver Operating Characteristic (ROC)</li> <li>Confusion Matrix</li> </ul> </ul> </details> <details> <summary><b>ML &amp; Deep Learning Frameworks</b></summary> <ul> <li>Scikit-Learn</li> <li>PyTorch</li> <li>TensorFlow</li> <li>JAX</li> <li>Resources</li> <ul> <li><a href="https://www.coursera.org/specializations/deep-learning">(Class) Deep Learning Specialisation by Andrew Ng</a></li> <li><a href="https://www.deeplearningbook.org/">(Book) Deep Learning by Ian Goodfellow</a></li> <li><a href="https://www.oreilly.com/library/view/hands-on-machine-learning/9781098125967/">(Book) Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 3rd Edition by Aurélien Géron</a></li> <li><a href="https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ">(Videos) Neural Networks: Zero to Hero by Andrej Karpathy</a></li> </ul> </ul> </details> <details> <summary><b>Model Development Lifecycle (MDLC)</b></summary> <ul> <li>Understand the end-to-end process of building, testing, deploying, and monitoring machine learning models.</li> </ul> </details> <details> <summary><b>Machine Learning Operations (MLOps)</b></summary> <ul> <li>Learn the principles and practices of maintaining and scaling ML workflows in production environments.</li> <li>General Resources</li> <ul> <li><a href="https://www.oreilly.com/library/view/designing-machine-learning/9781098107956/">(Book) Designing Machine Learning Systems by Chip Huyen</a></li> <li><a href="https://github.com/GokuMohandas/Made-With-ML">(Class) Made With ML</a></li> </ul> </ul> </details> <details> <summary><b>ML Pipelines</b></summary> <ul> <li>Pipeline Phases</li> <ul> <li>Data Preparation</li> <ul> <li>Data Extraction</li> <li>Data Analysis</li> <li>Data Preparation</li> </ul> <li>Model Development</li> <ul> <li>Model Training</li> <li>Model Evaluation</li> <li>Model Validation</li> </ul> <li>Model Serving</li> <ul> <li>Model Registry</li> <li>Model Prediction</li> <li>Model Performance Monitoring</li> </ul> </ul> </ul> </details> <details> <summary><b>Python</b></summary> <ul> <li>Learn object-oriented programming (OOP) principles.</li> <li>Proficiency in Pandas and NumPy for data manipulation.</li> <li>Use Jupyter notebooks for exploration and experimentation.</li> </ul> </details> <details> <summary><b>SQL</b></summary> <ul> <li>Ensure fluency in querying and manipulating structured data from relational databases.</li> </ul> </details> <details> <summary><b>Distributed Computing &amp; Big Data Processing</b></summary> <ul> <li>Leverage distributed computing for large-scale data processing.</li> <li>Use PySpark for writing scalable, Python-based ETL and analysis pipelines.</li> </ul> </details> <details> <summary><b>Data Warehouses &amp; Lakehouses</b></summary> <ul> <li><a href="https://www.databricks.com/" target="_blank" rel="noopener noreferrer">Databricks</a></li> <li><a href="https://www.snowflake.com/" target="_blank" rel="noopener noreferrer">Snowflake</a></li> <li><a href="https://cloud.google.com/bigquery" target="_blank" rel="noopener noreferrer">GCP BigQuery</a></li> <li><a href="https://aws.amazon.com/redshift/" target="_blank" rel="noopener noreferrer">Amazon Redshift</a></li> <li><a href="https://azure.microsoft.com/en-us/products/synapse-analytics" target="_blank" rel="noopener noreferrer">Azure Synapse Analytics</a></li> </ul> </details> <hr/> <h3 id="gen-ai--foundation-models">Gen AI &amp; Foundation Models</h3> <details> <summary><b>Cloud Model APIs</b></summary> <ul> <li><a href="https://azure.microsoft.com/en-us/products/ai-model-catalog">Azure - AI Foundry</a></li> <li><a href="https://cloud.google.com/model-garden">GCP - Vertex AI Model Garden</a></li> <li><a href="https://aws.amazon.com/bedrock/">AWS - Amazon Bedrock</a></li> <li><a href="https://openai.com/api/">OpenAI</a></li> </ul> </details> <details> <summary><b>Transformer Theory</b></summary> <ul> <li>Architecture &amp; modeling</li> <ul> <li>Attention mechanism</li> <li>Positional encoding</li> <li>Tokenization &amp; vector embeddings</li> <li>Decoder-only, encoder-only, and encoder-decoder architectures</li> <li>Hyperparameters</li> <ul> <li>Temperature</li> <li>Top-K</li> <li>Top-P</li> </ul> </ul> <li>Resources:</li> <ul> <li><a href="https://www.manning.com/books/build-a-large-language-model-from-scratch">(Book) Build a Large Language Model (From Scratch) by Sebastian Raschka</a></li> <li><a href="https://www.oreilly.com/library/view/hands-on-large-language/9781098150952/">(Book) Hands-On Large Language Models by Jay Alammar</a></li> <li><a href="https://www.oreilly.com/library/view/natural-language-processing/9781098136789/">(Book) Natural Language Processing with Transformers by Lewis Tunstall</a></li> <li><a href="https://www.youtube.com/watch?v=7xTGNNLPyMI&amp;ab_channel=AndrejKarpathy">(Video) Deep Dive into LLMs like ChatGPT by Andrej Karpathy</a></li> <li><a href="https://arxiv.org/abs/1706.03762">(Paper) Attention is All You Need</a></li> <li><a href="https://www.youtube.com/watch?v=9vM4p9NN0Ts&amp;ab_channel=StanfordOnline">(Class) Stanford CS229 - Machine Learning - Building Large Language Models (LLMs)</a></li> </ul> </ul> </details> <details> <summary><b>Gen AI Scaling Laws</b></summary> <ul> <li>Pre-training Scaling (parameter, data, and training compute size)</li> <li>Post-training Scaling (RLHF)</li> <li>Test-time Scaling/Inference Scaling</li> <li>Multi-Agent Scaling</li> </ul> </details> <details> <summary><b>Diffusion Models</b></summary> <ul> <li></li> </ul> </details> <details> <summary><b>Generative Adversarial Networks (GAN)</b></summary> <ul> <li><a href="https://www.deeplearning.ai/courses/generative-adversarial-networks-gans-specialization/" target="_blank" rel="noopener noreferrer">(Course) DeepLearning.AI - GAN Specialization</a></li> <li><a href="https://arxiv.org/abs/1406.2661" target="_blank" rel="noopener noreferrer">(Paper) Original GAN Paper (Goodfellow et al.)</a></li> <li><a href="https://www.tensorflow.org/tutorials/generative/dcgan" target="_blank" rel="noopener noreferrer">(Tutorial) TensorFlow Deep Convolutional GAN Tutorial</a></li> <li><a href="https://www.youtube.com/watch?v=8L11aMN5KY8" target="_blank" rel="noopener noreferrer">(Video) A Friendly Introduction to GANs by Serrano Academy</a></li> </ul> </details> <details> <summary><b>LLM Fine-Tuning</b></summary> <ul> <li>Compute efficiency techniques</li> <ul> <li>LoRA</li> <li>QLoRA</li> <li>PEFT</li> </ul> </ul> </details> <details> <summary><b>LLM Benchmarks</b></summary> <ul> <li><a href="https://www.latent.space/p/benchmarks-101">(Podcast) AI Fundamentals: Benchmarks 101</a></li> <li><a href="https://www.latent.space/p/benchmarks-201">(Podcast) Benchmarks 201: Why Leaderboards &gt; Arenas &gt;&gt; LLM-as-Judge</a></li> </ul> </details> <details> <summary><b>LLM Evaluation</b></summary> <ul> <li>LLM evaluation metrics</li> <ul> <li>Statistical metrics</li> <ul> <li><a href="https://en.wikipedia.org/wiki/BLEU">BLEU</a></li> <li><a href="https://en.wikipedia.org/wiki/ROUGE_(metric)">ROUGE (Recall-Oriented Understudy for Gisting Evaluation)</a></li> <li><a href="https://en.wikipedia.org/wiki/METEOR">METEOR (Metric for Evaluation of Translation with Explicit Ordering)</a></li> <li><a href="https://en.wikipedia.org/wiki/Levenshtein_distance">Levenshtein Distance</a></li> </ul> <li>LLM-as-judge metrics</li> </ul> <li>LLM evaluation tools</li> <ul> <li><a href="https://docs.ragas.io">Ragas</a></li> </ul> <li>LLM-as-a-judge techniques</li> <ul> <li>Pairwise comparison</li> <li>Evaluation by criteria (reference free)</li> <li>Evaluation by criteria (reference-based)</li> </ul> </ul> </details> <details> <summary><b>LLM Observability</b></summary> <ul> <li>Langfuse</li> <li><a href="https://www.langchain.com/langsmith">LangSmith</a>: a developer platform for inspecting, tracing, and evaluating LLM-powered applications built with LangChain or other orchestration frameworks. It enables fine-grained logging of prompts, model inputs/outputs, tool invocations, and intermediate steps, while supporting automated and manual evaluation workflows for performance, latency, and correctness.</li> </ul> </details> <details> <summary><b>AI Interpretability</b></summary> <ul> <li>Interpretability Methods</li> <ul> <li>Post-hoc Explainability</li> <li>Intrinsic Interpretability</li> <li>Mechanistic Interpretability</li> </ul> <li>Anthropic's Interpretability Team Publications</li> <ul> <li><a href="https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html">Dictionary Learning</a></li> <li><a href="https://www.anthropic.com/research/towards-monosemanticity-decomposing-language-models-with-dictionary-learning">Monosemanticity</a></li> <li><a href="https://transformer-circuits.pub/2025/attribution-graphs/biology.html">Attributional Graphs</a></li> </ul> </ul> </details> <details> <summary><b>LLM Safety &amp; Security</b></summary> <ul> <li>Content filtering</li> <ul> <li><a href="https://cloud.google.com/security-command-center/docs/model-armor-overview">GCP Model Armor</a> </li> </ul> </ul> </details> <details> <summary><b>Prompt/Context Engineering</b></summary> <ul> <li>Chain-of-Thought (CoT) Prompting</li> <li>ReAct</li> <li>Tree-of-Thoughts (ToT) Prompting</li> <li><a href="https://platform.openai.com/docs/guides/text?api-mode=responses">(Article) OpenAI Prompting Guide</a></li> <li><a href="https://www.promptingguide.ai/">(Website) Prompt Engineering Guide by DAIR.AI</a></li> </ul> </details> <details> <summary><b>AI Engineering</b></summary> <ul> <li><a href="https://www.oreilly.com/library/view/ai-engineering/9781098166298/">(Book) AI Engineering: Building Applications with Foundation Models by Chip Huyen</a></li> </ul> </details> <hr/> <h3 id="retrieval-augmented-generation">Retrieval Augmented Generation</h3> <details> <summary><b>RAG Fundamentals</b></summary> <ul> <li>Vector embeddings</li> <li>Chunking</li> <li>Hybrid retrieval</li> <li>General resources</li> <ul> <li> <a href="https://github.com/NirDiamant/RAG_Techniques">(GitHub) RAG Techniques by Nir Diamant</a> </li> </ul> </ul> </details> <details> <summary><b>RAG Evaluation</b></summary> <ul> <li>RAG evaluation metrics</li> <ul> <li>Context Precision</li> <li>Context Recall</li> <li>Content Entities Recall</li> <li>Noise Sensitivity</li> <li>Response Relevance</li> <li>Faithfulness</li> <li>Multimodal Faithfulness</li> <li>Multimodal Relevance</li> </ul> <li>RAG Eval Tools</li> <ul> <li><a href="https://docs.ragas.io/en/stable/">Ragas</a></li> </ul> </ul> </details> <details> <summary><b>Vector Databases</b></summary> <ul> <li>Vector search prototyping libraries</li> <ul> <li><a href="https://faiss.ai/">FAISS</a></li> <li><a href="https://github.com/nmslib/hnswlib">HNSWlib</a></li> </ul> <li>Production databases</li> <ul> <li><a href="https://www.pinecone.io/">Pinecone</a></li> <li><a href="https://weaviate.io/">Weaviate</a></li> <li><a href="https://www.trychroma.com/">Chroma</a></li> <li><a href="https://www.elastic.co/elasticsearch">Elasticsearch</a></li> <li><a href="https://milvus.io/">Milvus</a></li> </ul> </ul> </details> <details> <summary><b>Agentic RAG</b></summary> </details> <hr/> <h3 id="agentic-ai">Agentic AI</h3> <details> <summary><b>Agent Fundamentals</b></summary> <ul> <li>Resources</li> <ul> <li><a href="https://www.kaggle.com/whitepaper-agents">(White Paper) Google Agents White Paper by Julia Wiesinger et al.</a></li> <li><a href="https://www.kaggle.com/whitepaper-agent-companion">(White Paper) Google Agents Companion by Antonio Gulli et al.</a></li> <li><a href="https://arxiv.org/abs/2505.10468">(Paper) AI Agents vs. Agentic AI: A Conceptual Taxonomy, Applications and Challenges</a></li> <li><a href="https://arxiv.org/abs/2210.03629">(Paper) ReAct: Synergizing Reasoning and Acting in Language Models by Shunyu Yao et al.</a></li> <li><a href="https://huggingface.co/learn/agents-course/en/unit0/introduction">(Course) HuggingFace AI Agents Course</a></li> </ul> </ul> </details> <details> <summary><b>Agent Evaluation</b></summary> <ul> <li>Resources</li> <ul> <li><a href="https://arxiv.org/abs/2410.10934">(Paper) Agent-as-a-Judge: Evaluate Agents with Agents</a></li> </ul> </ul> </details> <details> <summary><b>Agent Frameworks</b></summary> <ul> <li><a href="https://www.langchain.com/">LangChain</a>: open-source framework for building agents</li> <li><a href="https://www.langchain.com/">LangGraph</a>: a newer-graph-based library within the LangChain ecosystem designed for creating complex, stateful, and multi-agent workflows with explicit state management and the ability to handle loops and cycles.</li> <ul> <li><a href="https://www.deeplearning.ai/short-courses/ai-agents-in-langgraph/">(Class) AI Agents in LangGraph by DeepLearning.AI</a></li> </ul> <li><a href="https://www.llamaindex.ai/">LlamaIndex</a>: specializes in efficiently indexing and querying large datasets for Retrieval Augmented Generation (RAG) applications</li> <ul> <li><a href="https://www.deeplearning.ai/short-courses/building-agentic-rag-with-llamaindex/">(Class) Building Agentic RAG with LlamaIndex</a></li> </ul> <li><a href="https://openai.github.io/openai-agents-python/">OpenAI Agent SDK</a>: OpenAI's framework for building agentic AI apps in a lightweight, easy-to-use package with few abstractions. It's a production-ready upgrade of their previous experimentatal framework (Swarm).</li> <li><a href="https://google.github.io/adk-docs/">Google Agent Development Kit (ADK)</a></li> <ul> <li><a href="https://codelabs.developers.google.com/onramp/instructions#0">(Code Lab) ADK Crash Course - From Beginner To Expert</a></li> </ul> </ul> </details> <details> <summary><b>Agent Protocols</b></summary> <ul> <li><a href="https://modelcontextprotocol.io/docs/getting-started/intro">Anthropic's Model Context Protocol (MCP)</a></li> <li><a href="https://a2a-protocol.org/latest/">Google's Agent-2-Agent (A2A)</a></li> <li><a href="https://cloud.google.com/blog/products/ai-machine-learning/announcing-agents-to-payments-ap2-protocol">Google's Agent Payments Protocol (AP2)</a></li> </ul> </details> <details> <summary><b>AI Integrated Development Environments (IDEs)</b></summary> <ul> <li><a href="https://cursor.com/agents">Cursor</a></li> <li><a href="https://windsurf.com/">Windsurf</a></li> <li><a href="https://replit.com/">Replit</a></li> <li><a href="https://openai.com/codex/">OpenAI Codex</a></li> <li><a href="https://claude.com/product/claude-code">Claude Code</a></li> <li><a href="https://codeassist.google/">Gemini Code Assist</a></li> </ul> </details> <details> <summary><b>Agentic Design Patterns</b></summary> <ul> <li><a href="https://www.philschmid.de/agentic-pattern">(Article) Zero to One: Learning Agentic Patterns</a></li> <li><a href="https://www.deeplearning.ai/short-courses/ai-agentic-design-patterns-with-autogen/">(Class) AI Agentic Design Patterns with AutoGen</a></li> <li><a href="https://www.deeplearning.ai/short-courses/multi-ai-agent-systems-with-crewai/">(Class) Multi AI Agent Systems with crewAI</a></li> </ul> </details> <hr/> <h3 id="general-technical-skills">General Technical Skills</h3> <details> <summary><b>Public Cloud Infrastructure</b></summary> <ul> <li><a href="https://cloud.google.com/">Google Cloud Platform (GCP)</a></li> <li><a href="https://azure.microsoft.com/en-us/">Microsoft Azure</a></li> <li><a href="https://aws.amazon.com/">Amazon Web Services (AWS)</a></li> </ul> </details> <details> <summary><b>Cloud FinOps</b></summary> <ul> <li>Manage cloud financial operations to maximize efficiency and optimize cost.</li> </ul> </details> <details> <summary><b>Data Pipelines</b></summary> <ul> <li>Pipeline Technologies</li> <ul> <li>Apache Airflow</li> <ul> <li>GCP Composer</li> <li>Amazon Managed Workflows for Apache Airflow (MWAA)</li> <li>Azure Workflow Orchestration Manager</li> </ul> <li>Apache Beam</li> <ul> <li>GCP Dataflow</li> </ul> <li>AWS Glue</li> <li>Apache Kafka</li> <li>Kubeflow Pipelines (KFP)</li> <li>TensorFlow Extended (TFX)</li> </ul> </ul> </details> <details> <summary><b>API and Backend Skills</b></summary> <ul> <li>Develop backends with FastAPI or Flask</li> <li>Implement REST or GraphQL streaming endpoints for AI services</li> <li>Design authentication and rate-limiting systems</li> <li>Build WebSocket implementations for real-time AI interactions</li> </ul> </details> <details> <summary><b>Data Validation</b></summary> <ul> <li><a href="https://docs.pydantic.dev/latest/">Pydantic</a></li> </ul> </details> <hr/> <h3 id="general-product-management-skills">General Product Management Skills</h3> <details> <summary><b>Project Management Frameworks</b></summary> <ul> <li><ins>Waterfall</ins>: A traditional, sequential approach where each project phase is completed before the next begins. Each phase has specific deliverables and a review process, making it suitable for projects with clearly defined requirements and predictable outcomes. However, it offers limited flexibility for changes once a phase is complete.</li> <li><ins>Agile</ins>: An iterative and incremental approach, suitable for projects with evolving requirements</li> <ul> <li><ins>Scrum</ins>: structured roles, sprints, and ceremonies.</li> <li><ins>Kanban</ins>: visual flow-based system emphasizing WIP limits and continuous delivery.</li> </ul> <li><ins>Lean</ins>: </li> </ul> </details> <details> <summary><b>Continuous Integration / Continuous Testing / Continuous Delivery (CI/CT/CD)</b></summary> <ul> <li><ins>Continuous Integration (CI)</ins>: Devs frequently merge code changes into a central repo, triggering automated builds to find integration issues quickly.</li> <li><ins>Continuous Testing (CT)</ins>: Automated tests (unit, integration, etc.) run continuously throughout the pipeline, verifying code quality and preventing bugs from reaching production.</li> <li><ins>Continuous Delivery (CD)</ins>: automatically prepares code for release, ensuring it's always in a deployable state, ready to be pushed to production with a single click.</li> </ul> </details> <details> <summary><b>DevOps and Site Reliability Engineering (SRE)</b></summary> <ul> <li>Bridge development and operations to ensure scalable, stable, and reliable systems.</li> <li>SRE focuses on uptime, latency, monitoring, and incident response with a software engineering mindset.</li> </ul> </details> <details> <summary><b>Product Metrics</b></summary> <ul> <li>User Adoption &amp; Engagement</li> <ul> <li><ins>Daily/Monthly Active Users (DAU/MAU)</ins>: the number of unique users who interact with the system daily/monthly</li> <li><ins>DAU/MAU Stickiness</ins>: measures how frequently users return to the product (higher ratios = better)</li> <li><ins>Feature Adoption Rate</ins>: percentage of users who use a specific feature within a given time period</li> </ul> <li>Customer Satisfaction</li> <ul> <li><ins>Net Prompter Score (NPS)</ins>: measures customer loyalty and satisfaction by asking users the likelihood of product recommendations</li> <li><ins>Customer Satisfaction Score (CSAT)</ins>: measures customer happiness with a specific feature or interaction</li> <li><ins>System Usability Scale (SUS)</ins>: measures the perceived usability of a product or system (range: 0-100 with higher scores indicating better usability). Importanlty, SUS doesn't measure the satisfaction with the product, so SUS + NPS is a good strategy.</li> <li><ins>Support Ticket Ratio</ins>: number of support tickets per user. A high number can signal product friction, bugs, or adoption issues.</li> </ul> <li>Retention &amp; Churn</li> <ul> <li><ins>Churn Rate</ins>: percentage of customers who stop using the product within a given timeframe</li> <li><ins>Retention Rate</ins>: percentage of customers who continue to use the product over a specific period. Retention is important as it's generally more cost efficient than new customer acquisitions</li> </ul> <li>Financial Performance</li> <ul> <li><ins>Customer Acquisition Cost (CAC)</ins>: </li> <li><ins>Customer Lifetime Value (CLTV)</ins>: </li> <li><ins>Monthly Recurring Revenue (MRR)</ins>: </li> <li><ins>Annual Recurring Revenue (ARR)</ins>: </li> <li><ins>Average Revenue Per User (ARPU)</ins>: </li> </ul> <li>Product Performance &amp; Delivery</li> </ul> </details> <details> <summary><b>Robust Documentation</b></summary> <ul> <li>Ensure product documentation is clear, current, and accessible to cross-functional teams.</li> </ul> </details> <details> <summary><b>Stakeholder Management</b></summary> <ul> <li>Adept at influencing executives and building consensus in a constantly changing and fast-paced environment.</li> </ul> </details> <details> <summary><b>Expert Storytelling</b></summary> <ul> <li>Craft compelling product messaging and present effectively to diverse audiences.</li> </ul> </details> <details> <summary><b>Product Launch Experience</b></summary> <ul> <li>Know what to do at each product launch stage and how to execute effectively to get things over the finish line</li> </ul> </details> <details> <summary><b>Growth and Expansive Mindset</b></summary> <ul> <li>Foster a curiosity to learn, a growth mindset, a positive attitude, and a "kind human" policy.</li> </ul> </details> <details> <summary><b>User Journeys</b></summary> <ul> <li>Define clear user journeys aligned with a strategic AI product philosophy and a north star metric.</li> </ul> </details> <details> <summary><b>Time Management and Productivity</b></summary> <ul> <li><ins>Eisenhower Matrix</ins>: organizing tasks into four quadrants, divided up by “urgency” on one axis and “importance” on the other</li> <li><ins>Covey Matrix</ins>: uses the same grid as the Eisenhower Matrix, but focuses on long-term planning instead of on daily tasks.</li> <li><ins>Pomodoro Technique</ins>: Pomodoro means “tomato” in Italian, and this method is named for the tomato-shaped timers originally used for it. The technique is simple to follow, set a timer for 25 minutes, work, and then set it for a five-minute break. Repeat the process four times.</li> <li><ins>Flowtime Technique</ins>: Sometimes called the “Flomodoro,” this method tries to help workers break through to a flow state. Users set a plan for the day and divide up their goals into small, manageable tasks. From there, they work without timers, taking breaks as needed. It’s essentially a deconstructed version of the Pomodoro Technique</li> <li><ins>Eat the Frog</ins>: Mark Twain said if you have to eat a frog, do it first thing in the morning. This approach to work is straightforward: Always do the most difficult and dreaded task first. Avoid multitasking and home in on the single task early. Then, the rest of the day will seem breezy by comparison.</li> </ul> </details> <hr/> <h3 id="ai-specific-product-skills">AI Specific Product Skills</h3> <details> <summary><b>AI Product Sense</b></summary> <ul> <li>Understand what can, and importantly cannot, be solved by AI (i.e. AI is not a silver bullet, many processes and products are better served with non-AI solutions)</li> </ul> </details> <details> <summary><b>AI Experiment Design</b></summary> <ul> <li>Practice iterative hypothesis testing with quantitative evaluation. </li> <li>Lead with A/B test, user interviews, and user feedback loops where possible</li> </ul> </details> <details> <summary><b>AI Market Insight</b></summary> <ul> <li>Build a deep understanding of the AI market, its competitive landscape, and emerging trends</li> </ul> </details> <hr/> <p><br/></p> <h2 id="newsletters">Newsletters</h2> <hr/> <h3 id="newsletters---tech-news">Newsletters - Tech News</h3> <ul> <li><a href="https://www.deeplearning.ai/the-batch/tag/data-points/">Data Points</a>: a twice‑weekly series from DeepLearning.AI of the most important AI tools, model releases, research findings, and industry developments</li> <li><a href="https://www.dailyzaps.com/">Daily Zaps</a>: a daily high-level tech news roundup that trends more business than technical</li> <li><a href="https://www.technologyreview.com/topic/download-newsletter/">The Download from MIT Technology Review</a>: a daily high-level tech news roundup from MIT Tech Review</li> <li><a href="https://www.emergingtechbrew.com/">Tech Brew</a>: a punchy, daily roundup of general technology news from the editors of the popular Morning Brew newsletter.</li> </ul> <h3 id="newsletters---cloud-developer-programs">Newsletters - Cloud Developer Programs</h3> <ul> <li><a href="https://developers.google.com/newsletter">Google Developer Program</a>: stay up to date with the latest GCP releases and features</li> <li><a href="https://info.microsoft.com/ww-landing-sign-up-for-the-microsoft-source-newsletter.html">Microsoft.Source newsletter</a>: the curated monthly developer community newsletter provides the latest articles, documentation, and events.</li> <li><a href="https://builder.aws.com/">AWS Builder Center</a>: Connect with other builders, share solutions, influence AWS product development, and access useful content.</li> </ul> <h3 id="newsletters---engineering-deep-dives">Newsletters - Engineering Deep Dives</h3> <ul> <li><a href="https://thesequence.substack.com/">TheSequence</a>: A weekly series that does technical deep dives on the latest AI/ML techniques</li> <li><a href="https://www.deeplearning.ai/the-batch/">The Batch @ DeepLearning.AI</a>: a weekly deep dive from Stanford Professor Andrew Ng</li> <li><a href="https://mlops.substack.com/">The MLOps Newsletter</a>: technical with a specific focus on MLOps</li> <li><a href="https://towardsdatascience.com/category/the-variable/">The Variable</a>: a curated list of articles/tutorials from Towards Data Science</li> <li><a href="https://www.turingpost.com/subscribe?ref=WAGU23hEVa">Turing Post</a>: a weekly newsletter by Ksenia Se that covers AI and ML curated summaries of hundreds of industry developments, research insights, and historical context</li> <li><a href="https://www.newsletter.swirlai.com/">SwirlAI</a>: MLOps and data engineering focused newsletter with great visualizations</li> <li><a href="">ByteByteGo</a>: a weekly newsletter by Alex Xu that delivers concise system‑design deep dives and foundational tech explainers on complex distributed systems topics like Kubernetes, databases, CI/CD, and API design</li> </ul> <hr/> <p><br/></p> <h2 id="podcasts">Podcasts</h2> <hr/> <ul> <li><a href="https://podcasts.apple.com/us/podcast/practical-ai/id1406537385">Practical AI by Changelog</a></li> <li><a href="https://www.youtube.com/playlist?list=PLRRoCwK1ZTNCAZXXOswpIYQqzMgT4swsI">Inference by Turing Post</a></li> <li><a href="https://www.latent.space/podcast">Latent Space: The AI Engineer Podcast</a></li> <li><a href="https://ai-podcast.nvidia.com/">NVIDIA AI Podcast</a></li> <li><a href="https://www.nytimes.com/column/hard-fork">Hard Fork by the NYT</a></li> <li><a href="https://a16z.com/podcasts/a16z-podcast/">The a16z Podcast by Andreessen Horowitz</a></li> </ul> <hr/> <p><br/></p> <h2 id="people">People</h2> <hr/> <ul> <li>The “Godfathers of AI” <ul> <li><a href="http://yann.lecun.com/">Yann LeCun</a>: Chief AI Scientist at Meta and a pioneer in optical character recognition (OCR) and convolutional neural networks (CNN). A Turing Award winner and one of the three “Godfathers of AI”.</li> <li><a href="https://en.wikipedia.org/wiki/Geoffrey_Hinton">Geoffrey Hinton</a>: He is the University Professor Emeritus at the University of Toronto and former Google Brain lead. Over his career he has been at the forefront of many AI advancements such as backpropagation, AlexNet, and deep learning. In 2024, he was awarded the Nobel Prize in Physics for machine learning with artificial neural networks. He’s also known for fostering a rich graduate student pipeline with past students being Alex Krizhevsky, Ilya Sutskever, Yann LeCun, and many other luminaries. In May 2023, Hinton resigned from Google and started speaking out against the dangers of AI.</li> <li><a href="https://yoshuabengio.org/">Yoshua Bengio</a>: co-Turing Award winne in 2018 with Yann LeCun and Geoffrey Hinton for his work on deep learning. Bengio is the most-cited computer scientist globally (by both total citations and by h-index), and the most-cited living scientist across all fields (by total citations).</li> </ul> </li> <li><a href="https://en.wikipedia.org/wiki/Demis_Hassabis">Demis Hassabis</a>: CEO and co-founder of Google DeepMind. He was jointly awarded the Nobel Prize in Chemistry in 2024 for his work on AlphaFold and protein structure prediction.</li> <li><a href="https://en.wikipedia.org/wiki/Mustafa_Suleyman">Mustafa Suleyman</a>: CEO of Microsoft AI and former head of applied AI at Google DeepMind</li> <li><a href="https://karpathy.ai/">Andrej Karpathy</a>: Former director of Autopilot at Tesla, co-founder of OpenAI, and prolific AI educator.</li> <li><a href="https://en.wikipedia.org/wiki/Fei-Fei_Li">Fei-Fei Li</a>: Stanford CS professor, co-director of the Stanford Institute for Human-Centered AI, inventor of ImageNet, and former Chief Scientist of AI/ML at GCP.</li> <li><a href="https://eugeneyan.com/subscribe">Eugene Yan</a>: ML, RecSys, LLMs, and engineering</li> <li><a href="https://www.andrewng.org/">Andrew Ng</a>: founder of Coursera, DeepLearning.AI, Stanford AI computer science professor, and neural network pioneer</li> </ul> <hr/> <p><br/></p> <h2 id="books">Books</h2> <hr/> <h3 id="pop-tech---general">Pop Tech - General</h3> <p>Narrative and investigative books about major tech companies, case studies, and industry shifts. These works build intuition about how technology intersects with power, markets, and society; helping PMs think strategically beyond the model and anticipate real-world consequences.</p> <h4 id="geopolitics">Geopolitics</h4> <ul> <li><a href="https://www.simonandschuster.com/books/Apple-in-China/Patrick-McGee/9781668053379">Apple in China: The Capture of the World’s Greatest Company</a> by Patrick McGee</li> <li><a href="https://www.simonandschuster.com/books/Chip-War/Chris-Miller/9781982172015">Chip War: The Fight for the World’s Most Critical Technology</a> by Chris Miller</li> <li><a href="https://bookriot.com/books/ai-valley-microsoft-google-and-the-trillion-dollar-race-to-cash-in-on-artificial-intelligence/">AI Valley: Microsoft, Google, and the Trillion-Dollar Race to Cash In on Artificial Intelligence – A Definitive Insider Chronicle of the Breakthroughs Redefining Our World</a> by Gary Rivlin</li> </ul> <h4 id="artificial-general-intelligence-agi">Artificial General Intelligence (AGI)</h4> <ul> <li><a href="https://technicspub.com/path-to-agi/">The Path to AGI: Artificial General Intelligence</a> by John K. Thompson</li> <li><a href="https://us.macmillan.com/books/9781250355027/theintelligenceexplosion/">The Intelligence Explosion: When AI Beats Humans At Everything</a> by James Barrat</li> <li><a href="https://www.superagency.ai/">Superagency: What Could Possibly Go Right with Our AI Future</a> by Reid Hoffman and Greg Beato</li> </ul> <h3 id="pop-tech---biographies--memoirs">Pop Tech - Biographies &amp; Memoirs</h3> <p>First-person and biographical accounts of influential technology leaders and builders. These books reveal how individual decisions, incentives, and leadership styles shape products and companies, helping AI Product Managers understand how human judgment, culture, and power influence technological outcomes.</p> <ul> <li><a href="https://wwnorton.com/books/the-nvidia-way">The Nvidia Way: Jensen Huang and the Making of a Tech Giant</a> by Tae Kim</li> <li><a href="https://en.wikipedia.org/wiki/Source_Code_(memoir)">Source Code: My Beginnings</a> by Bill Gates</li> </ul> <h3 id="product-management">Product Management</h3> <p>Foundational texts on building and scaling software products. They focus on execution, tradeoffs, and organizational dynamics, grounding AI product work in timeless principles for managing complexity and uncertainty.</p> <ul> <li><a href="https://en.wikipedia.org/wiki/The_Mythical_Man-Month">The Mythical Man-Month: Essays on Software Engineering</a> by Frederick Brooks, Jr.</li> </ul> <h3 id="data-science-textbooks">Data Science Textbooks</h3> <p>Technical books covering statistics, data analysis, and machine learning fundamentals. They give AI Product Managers the literacy needed to collaborate effectively with technical teams and make informed decisions about model capabilities and limitations.</p> <ul> <li><a href="https://www.manning.com/books/build-a-large-language-model-from-scratch">Build a Large Language Model (From Scratch)</a> by Sebastian Raschka</li> </ul>]]></content><author><name></name></author><category term="data-science"/><category term="AI"/><category term="Machine_Learning"/><category term="Deep_Learning"/><category term="Research"/><category term="Neural_Networks"/><category term="Product_Management"/><category term="Agents"/><summary type="html"><![CDATA[A guide to the AI product management landscape]]></summary></entry><entry><title type="html">RAG Engine Optimization: An Emerging Discipline</title><link href="https://duncancalvert.github.io/blog/2024/rag-engine-optimization-an-emerging-discipline/" rel="alternate" type="text/html" title="RAG Engine Optimization: An Emerging Discipline"/><published>2024-10-29T21:50:11+00:00</published><updated>2024-10-29T21:50:11+00:00</updated><id>https://duncancalvert.github.io/blog/2024/rag-engine-optimization-an-emerging-discipline</id><content type="html" xml:base="https://duncancalvert.github.io/blog/2024/rag-engine-optimization-an-emerging-discipline/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">How to Pass the Neo4j Certified Professional Exam</title><link href="https://duncancalvert.github.io/blog/2024/how-to-pass-the-neo4j-certified-professional-exam/" rel="alternate" type="text/html" title="How to Pass the Neo4j Certified Professional Exam"/><published>2024-08-04T19:52:57+00:00</published><updated>2024-08-04T19:52:57+00:00</updated><id>https://duncancalvert.github.io/blog/2024/how-to-pass-the-neo4j-certified-professional-exam</id><content type="html" xml:base="https://duncancalvert.github.io/blog/2024/how-to-pass-the-neo4j-certified-professional-exam/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">The Platonic Representation Hypothesis</title><link href="https://duncancalvert.github.io/blog/2024/the_platonic_representation_hypothesis/" rel="alternate" type="text/html" title="The Platonic Representation Hypothesis"/><published>2024-05-29T05:36:10+00:00</published><updated>2024-05-29T05:36:10+00:00</updated><id>https://duncancalvert.github.io/blog/2024/the_platonic_representation_hypothesis</id><content type="html" xml:base="https://duncancalvert.github.io/blog/2024/the_platonic_representation_hypothesis/"><![CDATA[<p>Amidst the buzz around GPT-4o and Google I/O, it was easy to miss this fascinating philosophical paper from MIT. “<a href="https://phillipi.github.io/prh/">The Platonic Representation Hypothesis</a>,” argues that AI models are converging towards a shared “reality”. As they grow in size, their data representations across different modalities (images, text, etc.) become more aligned, echoing Plato’s ideal forms.</p> <p><strong>Highlights:</strong></p> <ul> <li>Unified Representations: Larger models show more alignment, suggesting a common underlying reality and statistical model.</li> <li>Multimodal Efficiency: Combining data types boosts model performance and convergence.</li> <li>Revolutionary Potential: Easier cross-modal translation and adaptation could transform AI applications.</li> </ul>]]></content><author><name></name></author><category term="data-science"/><category term="AI"/><category term="MachineLearning"/><category term="DeepLearning"/><category term="MIT"/><category term="Research"/><category term="NeuralNetworks"/><summary type="html"><![CDATA[Exploring world representations in neural networks]]></summary></entry><entry><title type="html">Building Efficient Baselines in Minutes: EDA and Classification in Google Colab</title><link href="https://duncancalvert.github.io/blog/2023/building-efficient-baselines-in-minutes-eda-and-classification-in-google-colab/" rel="alternate" type="text/html" title="Building Efficient Baselines in Minutes: EDA and Classification in Google Colab"/><published>2023-11-26T23:23:35+00:00</published><updated>2023-11-26T23:23:35+00:00</updated><id>https://duncancalvert.github.io/blog/2023/building-efficient-baselines-in-minutes-eda-and-classification-in-google-colab</id><content type="html" xml:base="https://duncancalvert.github.io/blog/2023/building-efficient-baselines-in-minutes-eda-and-classification-in-google-colab/"><![CDATA[]]></content><author><name></name></author></entry></feed>